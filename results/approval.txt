Utility functions (direct mappings from a world state to an unbounded numerical value) are useful tools in a lot of ways, but they don't (seem to) describe how people reason about the world, partially because they go directly from world state to utility (which makes them impossible to usefully compute) and partially because they are hypothetically unbounded (which creates utility monsters and other, similar problems).

Agents can't reason about the world they exist in using world state directly except as a means of refining their model of the world, for fundamental mathematical reasons. Instead, they use descriptors - summaries of the world state that compress it down to manageable form. To avoid having to entirely recompute everything when the model is refined, descriptors are usually recursive. Low level descriptors feed into high level descriptors, and each level has its own associated dynamic model, so the agent can select which to use based on the scale of the action being considered. The descriptors can be described by a directed hypergraph with a source node representing the world state. Each edge of the hypergraph is a function that computes output descriptors from its input descriptors, but the hypergraph as a whole is not assumed to be a function with respect to any node it contains - the agent's computed value of each descriptor is (or may be) path dependent. Contradictions in the hypergraph allow model refinement, by one of three means: recalculation of underlying descriptors, refinement of descriptor state calculation functions, or refinement of the relevant dynamic model(s). In general though, the hypergraph is not fully computed when new data is received due to the opportunity cost associated with doing so; this means implicit contradictions can persist for some time.

Consider the idea of "approval relations" as a generalization of utility functions under this model. To the modelling hypergraph described above we add a sink node called "approval". Each edge entering the approval node maps a set of descriptor values to a binary output representing approval or disapproval of the universe state associated with the input descriptors. Each such edge is called an approval function; the approval relation is the entire structure mapping underlying universe state to approval/disapproval. An approval function is trivial if it maps all possible inputs to the same value. Trivially approved functions are discarded as irrelevant, and trivially disapproved functions are discarded as impossible (or incorporated into conditional approval functions, but I don't want to get sidetracked into a discussion of counterfactuals). The approval relation describes the agent's goal states, which are the set of states that map to approval (or, rather, that do not map to disapproval). In general the agent attempts to adjust the universe's state towards (an) approved state(s) or away from (a) disapproved state(s), increasing its confidence that the value of the approval node is positive. Because of the path dependence of descriptor values, any actual universe state may be both approved and disapproved by the agent, in which case it must perform the refinement operations discussed above in order to determine what action to take, but only if the agent is aware of the contradiction. It is also possible for an agent to pause refinement in order to take action, if the refinement process is determined to be less relevant than some limited opportunity action.
